# Elia's TIL

## 오늘 배운 내용
> Q. 시계열 데이터 분석이 데이터를 시간 기반으로 이해하는 데 중요한 이유를 설명하시오.

→ *시계열 데이터 분석은 시간을 중요한 요소로 고려하여 데이터의 패턴과 변화를 분석하는 방법임*

### 1. 시간에 따른 패턴 및 트렌드 분석 가능

- 데이터가 시간에 따라 어떻게 변화하는지를 파악할 수 있음
    - 인구수 : 저출산
    - 판매량 분석 : 명절 시즌에 판매량이 급증하는 경향
    

### 2. 계절성 및 주기적 변화 감지

- 특정 주기(일별,월별,계절별)로 반복되는 패턴을 찾을 수 있음.
    - 월별 : 국내 여행지 예약률 데이터를 분석하면, 여름휴가 시즌(7~8월)과 연말(12월)에 예약률이 가장 높음.
    - 계절별 : 여름철에는 에어컨, 아이스크림 판매량 급증, 겨울에는 난방기구 및 패딩 판매량 증가.

### 3. 미래 예측 가능

- 과거의 데이터를 기반으로 미래 값을 예측할 수 있음.
    - 저출산  →   향후 노동 인구 감소, 경제 성장 둔화 등 초래 가능 → **출산 장려 정책, 복지 시스템 강화**  마련
    - 여행지 성수기 비성수기 → 출발 날짜에 따라 금액대가 달라짐 → 이 데이터를 기반으로 숙박업체나 항공사에서는 성수기와 비수기를 구분하여 **가격 정책을 조정하거나 프로모션 전략을 세울 수 있음**

### 한줄정리

- 시계열 데이터 분석은 단순한 수치 비교가 아닌 시간의 흐름에 따른 데이터 변화를 파악하고 예측하여 대비할 수 있다.

> Q. 데이터 리모델링 패키지를 활용해 데이터를 변환하고 집계하는 방법을 설명하시오.

→ 데이터 리모델링은 데이터를 분석하기 적합한 형태로 변환하고, 특정 기준에 따라 집계하는 과정

→ 이를 위해 **Pandas, NumPy, dplyr (R)** 등의 패키지를 활용

### 데이터 변환

- 데이터 정규화 및 스케일링
    - 정규화(Normalization): 데이터 값을 일정한 범위(예: 0~1)로 변환하여 값의 크기에 의한 차이를 제거하는 과정.
    - 스케일링(Scaling): 서로 다른 단위를 가진 데이터의 크기를 조정하여 비교 가능하도록 만드는 과정.

```python
import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# 샘플 데이터 생성
df = pd.DataFrame({'Feature1': [100, 200, 300, 400, 500], 'Feature2': [5, 10, 15, 20, 25]})

# Min-Max Scaling 적용
scaler = MinMaxScaler()
df_minmax = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

# Standard Scaling 적용
scaler = StandardScaler()
df_standard = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

print(df_minmax)  # 0~1 범위로 변환된 데이터
print(df_standard)  # 평균 0, 표준편차 1로 변환된 데이터
```

- 데이터 피벗
    - 여러 개의 범주(Category)를 행과 열로 정리하여 분석 및 시각화를 쉽게 수행

```python
df = pd.DataFrame({
    'Date': ['2023-01', '2023-01', '2023-02', '2023-02'],
    'Category': ['A', 'B', 'A', 'B'],
    'Sales': [100, 150, 200, 250]
})

# 피벗 테이블 생성 (월별 카테고리별 매출 합계)
pivot_df = df.pivot_table(index='Date', columns='Category', values='Sales', aggfunc='sum')

print(pivot_df)
```

- dplyr
    - **R**에서 데이터 조작을 간단하고 직관적으로 수행할 수 있도록 도와주는 **tidyverse** 패키지 중 하나
    - **파이프 연산자 (%>%)** 를 활용
    
    ```python
    library(dplyr)
    
    # 예제 데이터 생성
    df <- data.frame(
      고객ID = c(1, 2, 3, 4, 5),
      지역 = c("서울", "부산", "서울", "대구", "부산"),
      매출 = c(100, 200, 150, 300, 250)
    )
    
    #특정 컬럼 선택
    df %>% select(고객ID, 매출)
    #매출이 높은 순서로 정렬
    df %>% arrange(desc(매출))
    #부가세 라는 새로운 변수 추가
    df %>% mutate(부가세 = 매출 * 0.1)
    ```
    

### 데이터 집계

- GroupBy

```python
df = pd.DataFrame({
    'Category': ['A', 'A', 'B', 'B', 'C'],
    'Sales': [100, 200, 300, 400, 500]
})

# 카테고리별 총 매출 계산
grouped_df = df.groupby('Category').agg({'Sales': 'sum'})

print(grouped_df)
```

### 한줄정리
- 데이터 리모델링 패키지를 활용한 변환 및 집계는 정규화·스케일링으로 데이터를 표준화하고, 피벗팅·그룹화를 통해 분석에 적합한 형태로 재구성하여 효과적인 인사이트를 도출하는 과정

 
 ## 오늘의 회고
- 챗지피티 특강을 통해 올바른 사용 방법을 알게 되었다. 질문을 잘해야겠다.
- 딥다이브를 통해 시계열 데이터에 대해 깊이 파고들 수 있었다. 미래를 예측하게 하여 문제를 방지할 수 있다는 점이
  매우 흥미로웠다.
